{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOV3IorLxxI-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ZOV3IorLxxI-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffe3ee23"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "id": "ffe3ee23"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Notice that different part of code may need different configration, according to different data preprocessing method"
      ],
      "metadata": {
        "id": "7zhLbOVKClaT"
      },
      "id": "7zhLbOVKClaT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read and analysis data"
      ],
      "metadata": {
        "id": "Cd0jYqU-CZeG"
      },
      "id": "Cd0jYqU-CZeG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVkFD7rvbe3p"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/comp4432/dont-overfit-ii/test.csv\")\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/comp4432/dont-overfit-ii/train.csv\")\n",
        "# Generate the column mapping dictionary\n",
        "column_mapping = {col: f'{col}' for col in test.columns}\n",
        "\n",
        "# Rename the columns using the mapping dictionary\n",
        "test.rename(columns=column_mapping, inplace=True)\n",
        "dataset.rename(columns=column_mapping, inplace=True)\n",
        "# display(test)z"
      ],
      "id": "MVkFD7rvbe3p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWfAurBwzxMA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of labels for the histogram\n",
        "labels = ['Test Samples', 'Training Samples']\n",
        "\n",
        "# Create a list of counts for each category\n",
        "counts = [test.shape[0], dataset.shape[0]]\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.bar(labels, counts)\n",
        "plt.xlabel('Sample Type')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Distribution of Training and Test Samples')\n",
        "plt.show()"
      ],
      "id": "hWfAurBwzxMA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmL3nMKWomaF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.sum(dataset[\"target\"] == 0)"
      ],
      "id": "pmL3nMKWomaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EalsmLC6Zmh"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# # Example DataFrame\n",
        "# data = pd.DataFrame({'Column1': [1, 4, 7, 3, 5, 2, 6, 8, 9, 3]})\n",
        "# i=0\n",
        "# j=0\n",
        "# fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(15, 15))\n",
        "# for col in dataset.columns[2:]:\n",
        "#   if(i>5):\n",
        "#     break\n",
        "#   if(j>=4):\n",
        "#     i+=1\n",
        "#     j%=4\n",
        "#   # Plot histogram of a column\n",
        "#   axes[i, j].hist(dataset[col])\n",
        "#   axes[i, j].set_title(col)\n",
        "#   j+=1\n",
        "#   # plt.ylabel('Frequency')\n",
        "#   # plt.title(col)\n",
        "#   # plt.show()"
      ],
      "id": "0EalsmLC6Zmh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO1Xz-bVHkVU"
      },
      "outputs": [],
      "source": [
        "dataset.describe().loc[\"std\"].nlargest(2)"
      ],
      "id": "nO1Xz-bVHkVU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2xhr19cSBe_"
      },
      "outputs": [],
      "source": [
        "dataset.describe().loc[\"std\"].min()"
      ],
      "id": "p2xhr19cSBe_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(test.drop(\"id\",axis=1).values.mean().sum()/300)\n",
        "print(test.drop(\"id\",axis=1).values.std().sum()/300)"
      ],
      "metadata": {
        "id": "iqQRXZcJMRuG"
      },
      "id": "iqQRXZcJMRuG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.mean().sum()/300)\n",
        "print(dataset.std().sum()/300)"
      ],
      "metadata": {
        "id": "nFWAicCoQlmH"
      },
      "id": "nFWAicCoQlmH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(\"id\", axis=1)\n",
        "test = test.drop(\"id\", axis=1)"
      ],
      "metadata": {
        "id": "ZoA80sCjXOFl"
      },
      "id": "ZoA80sCjXOFl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxvesulVZTzs"
      },
      "source": [
        "#*Training part(A compare the result of different simple models)"
      ],
      "id": "cxvesulVZTzs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale the dataset"
      ],
      "metadata": {
        "id": "hZ36qq11RS9x"
      },
      "id": "hZ36qq11RS9x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpBvqQ-6eLvi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# dataset_x = RobustScaler().fit_transform(np.concatenate((dataset.drop(\"target\",axis=1), test), axis=0))\n",
        "# test= dataset_x[250:]\n",
        "# dataset_x = dataset_x[:250]\n",
        "# #add 0.1 as noise to make the model not overfitting\n",
        "# dataset_x += np.random.normal(0, 0.1, dataset_x.shape)\n",
        "dataset_x = RobustScaler().fit_transform(dataset.drop(\"target\",axis=1))\n",
        "test = RobustScaler().fit_transform(test)\n",
        "dataset_x=pd.DataFrame(dataset_x)\n",
        "dataset_x[\"target\"]=dataset[\"target\"]\n",
        "dataset=dataset_x\n",
        "test.shape\n",
        "test=pd.DataFrame(test)"
      ],
      "id": "zpBvqQ-6eLvi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare train and test data"
      ],
      "metadata": {
        "id": "NuPTN3S1e-k6"
      },
      "id": "NuPTN3S1e-k6"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "train, validation = train_test_split(dataset, test_size=0.1, random_state=69)\n",
        "# train, validation = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_y=train[\"target\"]\n",
        "train_x=train.drop(\"target\",axis=1)\n",
        "val_y=validation[\"target\"]\n",
        "val_x=validation.drop(\"target\",axis=1)\n",
        "# # Identify minority class samples\n",
        "# minority_samples = train[train[\"target\"] == 1]\n",
        "\n",
        "# # Determine the desired number of duplicates\n",
        "# desired_duplicates = 100  # Choose the number of duplicates you want to create\n",
        "\n",
        "# # Create duplicates by randomly sampling from the minority class samples\n",
        "# duplicated_samples = minority_samples.sample(n=desired_duplicates, replace=True)\n",
        "\n",
        "# # Concatenate the duplicated samples with the original training set\n",
        "# train = pd.concat([train, duplicated_samples], axis=0, ignore_index=True).sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "B85OzTqvo6mm"
      },
      "id": "B85OzTqvo6mm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalization"
      ],
      "metadata": {
        "id": "2rH23x7-_qDG"
      },
      "id": "2rH23x7-_qDG"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "id": "X-ET2zIDqL7M"
      },
      "id": "X-ET2zIDqL7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature analysis"
      ],
      "metadata": {
        "id": "LE8s_vud7HM8"
      },
      "id": "LE8s_vud7HM8"
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install eli5\n",
        "import eli5\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a logistic regression model\n",
        "# model = ExtraTreesClassifier()\n",
        "model=LogisticRegression()\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "# # Use eli5 to get feature importance rankings\n",
        "# # feature_names = ['Feature A', 'Feature B', 'Feature C', 'Feature D', 'Feature E']\n",
        "feature_names = [str(feature) for feature in train_x.columns]\n",
        "feature_importances = eli5.show_weights(model, feature_names=feature_names,top=50)\n",
        "\n",
        "# # # Display the feature importance rankings\n",
        "display(feature_importances)\n",
        "# # Train a logistic regression model\n",
        "\n",
        "# # Use permutation importance from eli5 to get feature importances\n",
        "# perm = PermutationImportance(model).fit(train_x,train_y)\n",
        "# feature_importances = eli5.show_weights(perm)\n",
        "# display(eli5.show_weights(perm))\n",
        "\n",
        "# # # Get a binary mask of selected features based on importance threshold\n",
        "# # threshold = 0.1  # Adjust as needed\n",
        "# mask = perm.feature_importances_ > threshold\n",
        "\n",
        "# Select features based on the importance threshold\n",
        "# X_train_selected = SelectFromModel(model, threshold=threshold).fit_transform(train_x, train_y)\n",
        "# X_val_selected = SelectFromModel(model, threshold=threshold).transform(val_x)\n",
        "# X_test_selected = SelectFromModel(model, threshold=threshold).transform(test)\n",
        "# train_x=pd.DataFrame(X_train_selected)\n",
        "# val_x=pd.DataFrame(X_val_selected)\n",
        "# test=pd.DataFrame(X_test_selected)"
      ],
      "metadata": {
        "id": "HUnKWAcJjmGJ"
      },
      "id": "HUnKWAcJjmGJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "RIhZE94_cPre"
      },
      "id": "RIhZE94_cPre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resample"
      ],
      "metadata": {
        "id": "G413ZS-7War-"
      },
      "id": "G413ZS-7War-"
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "# Assuming 'X' is your feature matrix and 'y' is the corresponding label array\n",
        "smote = SMOTE(sampling_strategy='minority', n_jobs=-1)\n",
        "train_x, train_y = smote.fit_resample(train_x,train_y)\n",
        "print('Resampled dataset shape %s' % Counter(train_y))"
      ],
      "metadata": {
        "id": "EMdOikHmWZw5"
      },
      "id": "EMdOikHmWZw5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "id": "KvGVqnieac-m"
      },
      "id": "KvGVqnieac-m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YGQGP1f_pZt"
      },
      "id": "7YGQGP1f_pZt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "SJ_d4j9rJCVw"
      },
      "id": "SJ_d4j9rJCVw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ceb665jcZlmh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create an SVM model\n",
        "model = SVC()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(val_x)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(val_y, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "id": "Ceb665jcZlmh"
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(val_x)"
      ],
      "metadata": {
        "id": "n8RrBukQjaBh"
      },
      "id": "n8RrBukQjaBh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If9CEz7Hhu0H"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = dataset.drop(\"target\",axis=1)\n",
        "y = dataset[\"target\"]\n",
        "# model = ExtraTreesClassifier()\n",
        "model = LogisticRegression()\n",
        "# model = SVC()\n",
        "# model = KNeighborsClassifier()\n",
        "# model = RandomForestClassifier()\n",
        "# Define the number of folds for cross-validation\n",
        "k = 10\n",
        "\n",
        "# Perform K-fold cross-validation\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and compute the mean accuracy\n",
        "accuracy_scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
        "mean_accuracy = accuracy_scores.mean()\n",
        "# Perform K-fold cross-validation\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and compute the mean accuracy\n",
        "accuracy_scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
        "mean_accuracy = accuracy_scores.mean()\n",
        "\n",
        "# Print the accuracy for each fold and the mean accuracy\n",
        "i=1\n",
        "for score in accuracy_scores:\n",
        "  print(\"Accuracy for fold {} : {}\".format(i,score))\n",
        "  i+=1\n",
        "print(mean_accuracy)"
      ],
      "id": "If9CEz7Hhu0H"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ3GgdYFjW1h"
      },
      "id": "iZ3GgdYFjW1h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=model.predict(train_x)\n",
        "accuracy = accuracy_score(train_y, pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "display(pred)\n",
        "display(train_y.shape)"
      ],
      "metadata": {
        "id": "A_p7QjZW_78C"
      },
      "id": "A_p7QjZW_78C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection"
      ],
      "metadata": {
        "id": "lOTXe-ejAMi3"
      },
      "id": "lOTXe-ejAMi3"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yrEz3Cmdj03Q"
      },
      "id": "yrEz3Cmdj03Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_selection import RFE\n",
        "# from sklearn.datasets import make_regression\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# # Create an instance of the model you want to use (e.g., Linear Regression)\n",
        "# model = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.1)\n",
        "\n",
        "# # Create an RFE object and specify the model and number of features to select\n",
        "# rfe = RFE(estimator=model, n_features_to_select=50)\n",
        "\n",
        "# # Fit the RFE object to the data\n",
        "# rfe.fit(train_x, train_y)\n",
        "\n",
        "# # Get the selected features\n",
        "# selected_features = rfe.support_\n",
        "# selected_indices = [i for i, selected in enumerate(selected_features) if selected]\n",
        "\n",
        "# print(\"Selected Features:\")\n",
        "# for i in selected_indices:\n",
        "#     print(f\"Feature {i+1}\")\n",
        "# train_x = train_x.iloc[:, selected_indices]\n",
        "# val_x = val_x.iloc[:, selected_indices]\n",
        "# selected_test = test.drop(\"id\",axis=1).iloc[:, selected_indices]"
      ],
      "metadata": {
        "id": "-jFqpAFzAMCR"
      },
      "id": "-jFqpAFzAMCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply L1 regularization using Lasso\n",
        "lasso = Lasso(alpha=0.001)  # adjust the regularization strength with the alpha parameter\n",
        "lasso.fit(train_x, train_y)\n",
        "\n",
        "# Access the coefficients\n",
        "coefficients = lasso.coef_\n",
        "coefficients\n",
        "# Obtain the regularized dataset\n",
        "selected_features = np.where(lasso.coef_ != 0)[0]\n",
        "train_x = train_x.iloc[:, selected_features]\n",
        "val_x = val_x.iloc[:, selected_features]\n",
        "test_x =test.iloc[:, selected_features]"
      ],
      "metadata": {
        "id": "_D0uvv4aIT89"
      },
      "id": "_D0uvv4aIT89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataset is stored in a pandas DataFrame called 'data'\n",
        "correlation_matrix = train_x.corr()\n",
        "threshold = 0.5  # Set the correlation threshold\n",
        "\n",
        "# Find highly correlated feature pairs\n",
        "highly_correlated = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated.add(colname)\n",
        "# Drop the highly correlated features from the original dataset\n",
        "train_x = train_x.drop(highly_correlated, axis=1)\n",
        "test_x= test.drop(highly_correlated, axis=1)\n",
        "val_x = val_x.drop(highly_correlated, axis=1)"
      ],
      "metadata": {
        "id": "zcmkq4i4qz-S"
      },
      "id": "zcmkq4i4qz-S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Assuming your dataset is stored in a Pandas DataFrame called 'df'\n",
        "# and the target variable is named 'target'\n",
        "\n",
        "# Separate the features and target variable\n",
        "X = train_x\n",
        "y = train_y\n",
        "\n",
        "# Calculate the information gain (mutual information) for each feature\n",
        "information_gains = mutual_info_classif(X, y)\n",
        "\n",
        "# Create a DataFrame to store the feature names and their corresponding information gains\n",
        "feature_scores = pd.DataFrame({'Feature': X.columns, 'Information Gain': information_gains})\n",
        "\n",
        "# Sort the features based on their information gains in descending order\n",
        "sorted_features = feature_scores.sort_values('Information Gain', ascending=False)\n",
        "\n",
        "# Print the top 10 features with the highest information gain\n",
        "# top_features = sorted_features.head(150)\n",
        "# print(top_features)\n",
        "# Set a threshold to select the top features with high information gain\n",
        "threshold = 0.01  # Adjust the threshold as needed\n",
        "\n",
        "# Filter the features based on the threshold\n",
        "selected_features = sorted_features[sorted_features['Information Gain'] > threshold]\n",
        "\n",
        "# Obtain the final list of selected feature names\n",
        "selected_feature_names = selected_features['Feature'].tolist()\n",
        "\n",
        "# Print the selected features\n",
        "print(len(selected_feature_names))"
      ],
      "metadata": {
        "id": "JpbDnWGUS7KM"
      },
      "id": "JpbDnWGUS7KM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_x=val_x[selected_feature_names]\n",
        "train_x=train_x[selected_feature_names]\n",
        "selected_test=test.iloc[:,selected_feature_names]"
      ],
      "metadata": {
        "id": "tmN7wAvcuhat"
      },
      "id": "tmN7wAvcuhat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "8cdkkDfoz9L_"
      },
      "id": "8cdkkDfoz9L_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier elimination"
      ],
      "metadata": {
        "id": "N1KqO3o7dkJP"
      },
      "id": "N1KqO3o7dkJP"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the z-scores for each feature\n",
        "z_scores = (train_x - np.mean(train_x, axis=0)) / np.std(train_x, axis=0)\n",
        "\n",
        "# Set the threshold for outliers\n",
        "threshold = 3.7\n",
        "\n",
        "# Find the outliers for each attribute\n",
        "outliers = np.where(np.abs(z_scores) > threshold)\n",
        "\n",
        "# Plot the z-scores for each attribute\n",
        "plt.figure(figsize=(15, 20))\n",
        "plt.boxplot(z_scores)\n",
        "plt.xlabel('Attribute Index')\n",
        "plt.ylabel('Z-Score')\n",
        "plt.title('Z-Scores for Each Attribute')\n",
        "plt.show()\n",
        "plt.show()\n",
        "# Flatten the outliers indices\n",
        "outliers = np.any(np.abs(z_scores) > threshold, axis=1)\n",
        "# Remove the outliers from the dataset and labels\n",
        "train_x = train_x[~outliers]\n",
        "train_y = train_y[~outliers]"
      ],
      "metadata": {
        "id": "2udhBeMNdjWB"
      },
      "id": "2udhBeMNdjWB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highly_correlated"
      ],
      "metadata": {
        "id": "CMNj6ETRyFBy"
      },
      "id": "CMNj6ETRyFBy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y)\n",
        "y_pred = model.predict(val_x)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "LxdGQSt1dQ2l"
      },
      "id": "LxdGQSt1dQ2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMgc1dtuh3Kb"
      },
      "source": [
        "# Voting classifier(Train with network)"
      ],
      "id": "QMgc1dtuh3Kb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvjQq2mJUm9W"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_simple_nn(input_dim):\n",
        "    # Define the architecture of the neural network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=64, activation='relu', input_shape=(input_dim,)))  # Input layer\n",
        "    model.add(Dense(units=1, activation='sigmoid'))  # Output layer (one unit for binary classification)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "input_dim = 300\n",
        "\n",
        "# Build the simple neural network\n",
        "model = build_simple_nn(input_dim)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_x, train_y, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(val_x, val_y)"
      ],
      "id": "XvjQq2mJUm9W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to use a NN to get voting result"
      ],
      "metadata": {
        "id": "UrFKf4AM3UQQ"
      },
      "id": "UrFKf4AM3UQQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmMFpLt8fGK2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "# Convert the binary numbers to a 2D NumPy array\n",
        "train_encode = np.array(train_y)\n",
        "\n",
        "hot_train_y = [to_categorical(label, num_classes=2).tolist() for label in train_y]\n",
        "lr=0.1\n",
        "# Load your dataset (X and y) or use your own dataset\n",
        "X, y = train_x, train_y\n",
        "clf1 = LogisticRegression()\n",
        "clf2 = RandomForestClassifier()\n",
        "clf3 = SVC(probability=True)\n",
        "clf4 = ExtraTreesClassifier()\n",
        "clf5 = KNeighborsClassifier()\n",
        "#build the voting classififer\n",
        "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3),('Extree',clf4),('KNN',clf5)], voting='soft', weights=[1, 1, 0.5, 1, 0.3])\n",
        "# Define the number of folds for K-fold cross-validation\n",
        "k = 10\n",
        "\n",
        "# Initialize a list to store the accuracies for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Initialize the K-fold cross-validator\n",
        "kf = KFold(n_splits=k)\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_shape=(5,))) # Single layer with 2 output nodes\n",
        "model.add(Dense(units=128, activation='relu'))  # Output layer (one unit for binary classification)\n",
        "model.add(Dense(units=2, activation=\"softmax\"))  # Output layer (one unit for binary classification)\n",
        "# model.add(Dense(units=1, activation='sigmoid'))  # Output layer (one unit for binary classification)\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit(X, y, epochs=10, batch_size=32)\n",
        "\n",
        "# Make predictions\n",
        "# predictions = model.predict(X)\n",
        "# Iterate over the folds\n",
        "y_all=np.array([])\n",
        "stacked= np.empty((0, 5, 1))\n",
        "for train_index, val_index in kf.split(range(train.shape[0])):\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    result=[]\n",
        "    # print(train_index.shape)\n",
        "    # print(val_index.shape)\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model on the training set\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_pred = voting_clf.predict_proba(X_val)\n",
        "    for estimator in voting_clf.estimators_:\n",
        "      pred=estimator.predict_proba(X_val)[:,1]\n",
        "      result.append(pred)\n",
        "    # print(result)\n",
        "    result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "    stacked = np.concatenate((stacked, result), axis=0)\n",
        "    # print(stacked)\n",
        "    y_all = np.concatenate((y_all, y_val))\n",
        "# result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "# modified_result = np.hstack((result, np.ones((result.shape[0], 1))))[:, :,np.newaxis]\n",
        "hot_train_y = [to_categorical(label, num_classes=2).tolist() for label in y_all]\n",
        "model.fit(stacked, np.array(hot_train_y))\n",
        "\n",
        "# Calculate the average accuracy across all folds\n",
        "# average_accuracy = sum(accuracies) / k\n",
        "\n",
        "# Print the average accuracy\n",
        "# print(\"Average Accuracy:\", average_accuracy)"
      ],
      "id": "JmMFpLt8fGK2"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "result=[]\n",
        "for estimator in voting_clf.estimators_:\n",
        "  pred=estimator.predict_proba(train_x)[:,1]\n",
        "  result.append(pred)\n",
        "result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "pred=model.predict(result)\n",
        "# hot_train_y = [to_categorical(label, num_classes=2).tolist() for label in train_y]\n",
        "fpr, tpr, thresholds = roc_curve(np.argmax(pred, axis=1), train_y)\n",
        "\n",
        "# Compute the Area Under the ROC Curve (AUC-ROC)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G-KZzYtZAKkC"
      },
      "id": "G-KZzYtZAKkC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "EYrN8-MHSHNM"
      },
      "id": "EYrN8-MHSHNM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(stacked, np.array(hot_train_y))"
      ],
      "metadata": {
        "id": "tG7Tp8z3v7hS"
      },
      "id": "tG7Tp8z3v7hS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked.shape"
      ],
      "metadata": {
        "id": "zAGJLr1-7rVD"
      },
      "id": "zAGJLr1-7rVD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "9xVjCZpa8pvy"
      },
      "id": "9xVjCZpa8pvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(dataset[\"target\"]==0)"
      ],
      "metadata": {
        "id": "tdb2vZUbk8AR"
      },
      "id": "tdb2vZUbk8AR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "try to use panalty"
      ],
      "metadata": {
        "id": "MPZ6aWSPDiWf"
      },
      "id": "MPZ6aWSPDiWf"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier,GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import statistics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "lr=0.1\n",
        "# Load your dataset (X and y) or use your own dataset\n",
        "X, y = train_x, train_y\n",
        "clf1 = LogisticRegression(max_iter=100, class_weight='balanced', penalty='l2', C=0.1, solver='liblinear')\n",
        "# clf2 = RandomForestClassifier()\n",
        "clf2= LogisticRegression(class_weight='balanced', solver='liblinear', penalty ='l1', C= 0.1, max_iter=10000)\n",
        "clf3 = LogisticRegression(penalty=\"l2\",C=0.8)\n",
        "clf4 = GradientBoostingClassifier()\n",
        "# clf4 = GaussianNB()\n",
        "meta_classifier=GaussianNB()\n",
        "# clf5 = KNeighborsClassifier()\n",
        "clf5= SGDClassifier(eta0=1, max_iter=100, tol=0.0001, alpha=0.01, l1_ratio=0.0, learning_rate='adaptive', loss='log', penalty='elasticnet')\n",
        "#build the voting classififer\n",
        "# voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3),('Extree',clf4),('KNN',clf5)], voting='soft', weights=[1, 1, 1, 1, 1])\n",
        "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf5)], voting='soft', weights=[1, 1, 1])\n",
        "# Define the number of folds for K-fold cross-validation\n",
        "k = 20\n",
        "# voting_clf = RFE(estimator=clf2, n_features_to_select=150,importance_getter='feature_importances_')\n",
        "# Initialize a list to store the accuracies for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Initialize the K-fold cross-validator\n",
        "kf = KFold(n_splits=k,shuffle=True)\n",
        "result=[]\n",
        "y_all=np.array([])\n",
        "weight=[1.0,1.0,1.0]\n",
        "j=0\n",
        "print(\"The classsifiers are LogisticRegression, RFC, SVC, ExtraTreesClassifier, KNeighborsClassifier\")\n",
        "for train_index, val_index in kf.split(range(train.shape[0])):\n",
        "    j+=1\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    # print(train_index.shape)\n",
        "    # print(val_index.shape)\n",
        "    result=[]\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model on the training set\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    accuracies=[]\n",
        "    # y_pred = voting_clf.predict_proba(X_val)\n",
        "    for estimator in voting_clf.estimators_:\n",
        "      pred=estimator.predict_proba(X_val)[:,1]\n",
        "      result.append(pred)\n",
        "      # print(pred)\n",
        "      # print(y_val)\n",
        "      auc_roc = roc_auc_score(y_val, pred)\n",
        "      accuracies.append(auc_roc)\n",
        "    threshold_auc=statistics.median(accuracies)\n",
        "    penalty_factor = max(0, threshold_auc - auc_roc)\n",
        "    i=0\n",
        "    for auc_roc,w in zip(accuracies,weight):\n",
        "      penalty_factor = max(0, threshold_auc - auc_roc)\n",
        "      # penalized_auc_roc = auc_roc * penalty_factor\n",
        "      weight[i]=w-w * penalty_factor\n",
        "      i+=1\n",
        "    total = sum(weight)\n",
        "    # print(y_val)\n",
        "    normalized_weights = [w / total for w in weight]\n",
        "\n",
        "    rounded_numbers = []\n",
        "    for number in normalized_weights:\n",
        "        rounded_number = round(number, 2)\n",
        "        formatted_number = \"{:.2f}\".format(rounded_number)\n",
        "        rounded_numbers.append(formatted_number)\n",
        "    print(\"Fold {},the updated weight(normalized) are {}\".format(j,rounded_numbers))\n",
        "#   # Calculate the sum of the weights\n",
        "\n",
        "total = sum(weight)\n",
        "# Normalize the weights\n",
        "normalized_weights = [w / total for w in weight]\n",
        "voting_clf.weights=normalized_weights\n",
        "#       result.append(pred)\n",
        "#     y_all = np.concatenate((y_all, y_val))\n",
        "# result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "# modified_result = np.hstack((result, np.ones((result.shape[0], 1))))[:, :,np.newaxis]\n",
        "# model.fit(result, y_all)\n",
        "\n",
        "# Calculate the average accuracy across all folds\n",
        "# average_accuracy = sum(accuracies) / k\n",
        "\n",
        "# Print the average accuracy\n",
        "# print(\"Average Accuracy:\", average_accuracy)"
      ],
      "metadata": {
        "id": "SHmgVBq_DUYH"
      },
      "id": "SHmgVBq_DUYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Meta-classifier\n"
      ],
      "metadata": {
        "id": "I_Zo7OKbOSES"
      },
      "id": "I_Zo7OKbOSES"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier,GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import statistics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "lr=0.1\n",
        "# Load your dataset (X and y) or use your own dataset\n",
        "X, y = train_x, train_y\n",
        "clf1 = LogisticRegression(max_iter=100, class_weight='balanced', penalty='l2', C=0.1, solver='liblinear')\n",
        "# clf2 = RandomForestClassifier()\n",
        "clf2= LogisticRegression(class_weight='balanced', solver='liblinear', penalty ='l1', C= 0.1, max_iter=10000)\n",
        "clf3 = LogisticRegression(penalty=\"l2\",C=0.8)\n",
        "clf4 = ExtraTreesClassifier()\n",
        "# clf4 = GaussianNB()\n",
        "meta_classifier=GaussianNB()\n",
        "# clf5 = KNeighborsClassifier()\n",
        "clf5= SGDClassifier(eta0=1, max_iter=100, tol=0.0001, alpha=0.01, l1_ratio=0.0, learning_rate='adaptive', loss='log', penalty='elasticnet')\n",
        "#build the voting classififer\n",
        "# voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3),('Extree',clf4),('KNN',clf5)], voting='soft', weights=[1, 1, 1, 1, 1])\n",
        "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf5)], voting='soft', weights=[1, 1, 1])\n",
        "voting_clf.weights=normalized_weights\n",
        "# Define the number of folds for K-fold cross-validation\n",
        "k = 75\n",
        "# voting_clf = RFE(estimator=clf2, n_features_to_select=150,importance_getter='feature_importances_')\n",
        "# Initialize a list to store the accuracies for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Initialize the K-fold cross-validator\n",
        "kf = KFold(n_splits=k)\n",
        "result=[]\n",
        "y_all=np.array([])\n",
        "weight=[1.0,1.0,1.0,1.0,1.0]\n",
        "j=0\n",
        "print(\"The classsifiers are LogisticRegression, RFC, SVC, ExtraTreesClassifier, KNeighborsClassifier\")\n",
        "for train_index, val_index in kf.split(range(train.shape[0])):\n",
        "    j+=1\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    # print(train_index.shape)\n",
        "    # print(val_index.shape)\n",
        "    result=[]\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Train the model on the training set\n",
        "    # if(j==1):\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "    # else:\n",
        "      # for estimator in voting_clf.estimators_:\n",
        "        # estimator.partial_fit(X_train,y_train,classes=np.unique(y))\n",
        "    # Evaluate the model on the validation set\n",
        "    accuracies=[]\n",
        "    # y_pred = voting_clf.predict_proba(X_val)\n",
        "    i=0\n",
        "    for estimator in voting_clf.estimators_:\n",
        "      pred=estimator.predict_proba(X_val)[:,1]*voting_clf.weights[i]\n",
        "      i+=1\n",
        "      result.append(pred)\n",
        "    result1=np.transpose(np.array(result)).reshape(-1, 3)\n",
        "    if(j==1):\n",
        "      meta_classifier.fit(result1,y_val)\n",
        "    else:\n",
        "      meta_classifier.partial_fit(result1, y_val)\n",
        "'''\n",
        "Using AUC-ROC to do the weight updating\n",
        "'''\n",
        "#       auc_roc = roc_auc_score(val_y, pred)\n",
        "#       accuracies.append(auc_roc)\n",
        "#     threshold_auc=statistics.median(accuracies)\n",
        "#     penalty_factor = max(0, threshold_auc - auc_roc)\n",
        "#     i=0\n",
        "#     for auc_roc,w in zip(accuracies,weight):\n",
        "#       penalty_factor = max(0, threshold_auc - auc_roc)\n",
        "#       # penalized_auc_roc = auc_roc * penalty_factor\n",
        "#       weight[i]=w-w * penalty_factor\n",
        "#       i+=1\n",
        "#     total = sum(weight)\n",
        "#     # print(y_val)\n",
        "#     normalized_weights = [w / total for w in weight]\n",
        "\n",
        "#     rounded_numbers = []\n",
        "#     for number in normalized_weights:\n",
        "#         rounded_number = round(number, 2)\n",
        "#         formatted_number = \"{:.2f}\".format(rounded_number)\n",
        "#         rounded_numbers.append(formatted_number)\n",
        "#     print(\"Fold {},the updated weight(normalized) are {}\".format(j,rounded_numbers))\n",
        "#   # Calculate the sum of the weights\n",
        "\n",
        "# total = sum(weight)\n",
        "# # Normalize the weights\n",
        "# normalized_weights = [w / total for w in weight]\n",
        "# voting_clf.weights=normalized_weights\n",
        "#       result.append(pred)\n",
        "#     y_all = np.concatenate((y_all, y_val))\n",
        "# result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "# modified_result = np.hstack((result, np.ones((result.shape[0], 1))))[:, :,np.newaxis]\n",
        "# model.fit(result, y_all)\n",
        "\n",
        "# Calculate the average accuracy across all folds\n",
        "# average_accuracy = sum(accuracies) / k\n",
        "\n",
        "# Print the average accuracy\n",
        "# print(\"Average Accuracy:\", average_accuracy)"
      ],
      "metadata": {
        "id": "pSVHJqALOQqZ"
      },
      "id": "pSVHJqALOQqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "i=0\n",
        "for estimator in voting_clf.estimators_:\n",
        "  pred=estimator.predict_proba(val_x)[:,1]*voting_clf.weights[i]\n",
        "  i+=1\n",
        "  result.append(pred)\n",
        "result=np.transpose(np.array(result)).reshape(-1, 3)\n",
        "pred=meta_classifier.predict(result)\n",
        "accuracy = accuracy_score(val_y, pred)\n",
        "display(accuracy)\n",
        "display(val_y)\n",
        "display(pred)"
      ],
      "metadata": {
        "id": "0br-mvOzOzHk"
      },
      "id": "0br-mvOzOzHk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "O6W9i9uev7E6"
      },
      "id": "O6W9i9uev7E6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "for estimator in voting_clf.estimators_:\n",
        "  pred=estimator.predict(val_x)\n",
        "  result.append(pred)\n",
        "  accuracy = accuracy_score(val_y, pred)\n",
        "  print(accuracy)\n",
        "#   print(pred)\n",
        "# print(val_y)\n",
        "i=0\n",
        "vote=[0 for _ in range(len(train_y))]\n",
        "for r in result:\n",
        "  # print(r)\n",
        "  vote+=r*voting_clf.weights[i]\n",
        "pred=voting_clf.predict(val_x)\n",
        "accuracy = accuracy_score(val_y, pred)\n",
        "# print(accuracy)\n",
        "# result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "# # modified_result = np.hstack((result, np.ones((result.shape[0], 1))))[:, :,np.newaxis]\n",
        "# pred=voting_clf.predict(val_x)\n",
        "# pred\n",
        "# print(list(val_y,type=uint8))\n",
        "# display(accuracy)\n",
        "# display(pred)\n",
        "# display(list(val_y))\n",
        "print(accuracy)\n",
        "# print(y_pred)\n",
        "# print(val_y)"
      ],
      "metadata": {
        "id": "Z9ppHMOy3Lv4"
      },
      "id": "Z9ppHMOy3Lv4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "p2dliQFexeW_"
      },
      "id": "p2dliQFexeW_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_y"
      ],
      "metadata": {
        "id": "EwCoa6fNyV6g"
      },
      "id": "EwCoa6fNyV6g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "fpr, tpr, thresholds = roc_curve(voting_clf.predict(val_x), val_y)\n",
        "\n",
        "# Compute the Area Under the ROC Curve (AUC-ROC)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "print(y_pred)\n",
        "print(val_y)"
      ],
      "metadata": {
        "id": "WyZg_R_AZMtC"
      },
      "id": "WyZg_R_AZMtC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(voting_clf.predict(val_x))\n",
        "display(val_y)"
      ],
      "metadata": {
        "id": "4rz7my1g0-zY"
      },
      "id": "4rz7my1g0-zY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of prediction result"
      ],
      "metadata": {
        "id": "r-wnpny28px0"
      },
      "id": "r-wnpny28px0"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have computed the true labels and predicted probabilities\n",
        "# true_labels: true labels of the validation or test data\n",
        "# predicted_probs: predicted probabilities for the positive class\n",
        "\n",
        "for r in result:\n",
        "  # Compute the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds\n",
        "  fpr, tpr, thresholds = roc_curve(val_y, r)\n",
        "\n",
        "  # Compute the Area Under the ROC Curve (AUC-ROC)\n",
        "  auc_score = auc(fpr, tpr)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  plt.figure()\n",
        "  plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_score)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "MwhoSCTb7lH3"
      },
      "id": "MwhoSCTb7lH3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r"
      ],
      "metadata": {
        "id": "6TIdWPL1xlZD"
      },
      "id": "6TIdWPL1xlZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=[]\n",
        "for estimator in voting_clf.estimators_:\n",
        "  pred=estimator.predict_proba(test.drop(\"id\",axis=1))[:,1]\n",
        "  result.append(pred)\n",
        "result=np.transpose(np.array(result)).reshape(-1, 5,1)\n",
        "# modified_result = np.hstack((result, np.ones((result.shape[0], 1))))[:, :,np.newaxis]\n",
        "pred=model.predict(result)"
      ],
      "metadata": {
        "id": "PdkDOOQAvBsg"
      },
      "id": "PdkDOOQAvBsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(pred > 0.5)"
      ],
      "metadata": {
        "id": "xMyhLVal16l7"
      },
      "id": "xMyhLVal16l7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "y_pred = voting_clf.predict_proba(val_x)\n",
        "result=[]\n",
        "for estimator in voting_clf.estimators_:\n",
        "  pred=estimator.predict_proba(val_x)[:,1]\n",
        "  result.append(pred)\n",
        "result=np.transpose(np.array(result)).reshape(-1, 5, 1)\n",
        "model.predict(result)"
      ],
      "metadata": {
        "id": "d59MhqkLgpdw"
      },
      "id": "d59MhqkLgpdw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVzguQnRr1zZ"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have your feature matrix X and target variable y\n",
        "# Create individual classifiers\n",
        "clf1 = LogisticRegression(random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = SVC(random_state=42)\n",
        "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='hard', weights=[1, 1, 1])\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model\n",
        "model = svm.SVC(probability=True)  # Set probability=True to enable probability estimates\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the AUC-ROC\n",
        "auc_roc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"AUC-ROC:\", auc_roc)"
      ],
      "id": "EVzguQnRr1zZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRu0xP6ehlrd"
      },
      "source": [
        "# Obtain test prediction result"
      ],
      "id": "VRu0xP6ehlrd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoxSrvOIKy5O"
      },
      "outputs": [],
      "source": [
        "# Perform majority voting to get the final prediction\n",
        "# voting_predictions = voting_clf.predict(selected_test).flatten()\n",
        "\n",
        "# Prepare the submission file\n",
        "# submission = pd.DataFrame({'id': test['id'], 'target': voting_predictions})\n",
        "\n",
        "# Save the submission file as a CSV\n",
        "# submission.to_csv('/content/drive/MyDrive/Colab Notebooks/comp4432/dont-overfit-ii/submission.csv', index=False)\n",
        "# x_test=scaler.fit_transform(test.drop(\"id\",axis=1))\n",
        "result=[]\n",
        "i=0\n",
        "for estimator in voting_clf.estimators_:\n",
        "  pred=estimator.predict_proba(test_x)[:,1]*voting_clf.weights[i]\n",
        "  i+=1\n",
        "  result.append(pred)\n",
        "result=np.transpose(np.array(result)).reshape(-1, 3)\n",
        "pred=meta_classifier.predict_proba(result)[:,1]\n",
        "# pred = pd.DataFrame(pred)\n",
        "pred = pd.DataFrame(voting_clf.predict_proba(test_x)[:,1])\n",
        "pred.index += 250\n",
        "pred.columns = ['target']\n",
        "pred.to_csv('/content/drive/MyDrive/Colab Notebooks/comp4432/dont-overfit-ii/submission.csv', index_label='id', index=True)"
      ],
      "id": "BoxSrvOIKy5O"
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf.weights"
      ],
      "metadata": {
        "id": "XeVDUUvQhUY0"
      },
      "id": "XeVDUUvQhUY0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWOlx2YEXczR"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(voting_clf.predict(test_x) == 0)"
      ],
      "id": "xWOlx2YEXczR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0VF9bmoNWt1"
      },
      "outputs": [],
      "source": [
        "model.predict(test)"
      ],
      "id": "U0VF9bmoNWt1"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}